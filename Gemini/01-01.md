# 1.1. 言語モデルの定義と変遷

大規模言語モデル（Large Language Models: LLMs）の動作原理を理解するためには、まずその基盤となる「言語モデル（Language Model: LM）」の数学的定義と、それがどのように進化してきたかの歴史的経緯を整理する必要がある。

### 1.1.1. 言語モデルの数学的定義

言語モデルとは、簡潔に言えば「文（単語の系列）に対して確率を割り当てるモデル」である。

ある単語の集合（語彙）を  とし、そこから得られる長さ  の単語系列を  とする。言語モデルの目的は、この系列  がその言語において出現する妥当性を示す同時確率  を推定することにある。

この同時確率は、条件付き確率の連鎖律（Chain Rule）を用いることで、以下のように分解できる。

ここで、 は、過去の文脈（コンテキスト）  が与えられたときに、次に単語  が出現する条件付き確率を表す。現代の LLM の多くは、この形式で「次の単語を予測する」ことで学習を行う**自己回帰型（Autoregressive）言語モデル**である。

### 1.1.2. 言語モデルの歴史的変遷

言語モデルの歴史は、計算リソースの増大とアルゴリズムの進化に伴い、大きく 4 つの段階に分けることができる。

#### 1. 統計的言語モデル（Statistical Language Models: SLM）

1980年代から1990年代にかけて主流であったアプローチであり、代表的な手法に **N-gram モデル**がある。これはマルコフ仮定に基づき、直前の  個の単語のみに依存して次の単語を予測するものである。

* **特徴**: 出現頻度に基づく単純なカウントによって確率を算出する。
* **限界**: 長い依存関係を捉えられないこと（データスパースネス問題）や、単語間の意味的類似性を考慮できないという欠点があった。

#### 2. ニューラル言語モデル（Neural Language Models: NLM）

2000年代初頭、Bengio らによって提案された手法である。単語を連続的なベクトル空間に投影する**分散表現（Word Embedding）**を導入し、ニューラルネットワークを用いて条件付き確率をモデリングした。

* **特徴**: 単語間の意味的類似性を計算可能にし、N-gram よりも高い汎化性能を示した。
* **限界**: 初期段階では固定長の入力を扱っていたため、長い文のコンテキストを保持することに依然として課題があった。

#### 3. 事前学習済み言語モデル（Pre-trained Language Models: PLM）

2010年代後半、**Transformer** アーキテクチャの登場（2017年）により、この段階へと移行した。ELMo や BERT、初期の GPT などがこれに該当する。

* **特徴**: 大規模なコーパスを用いて「自己教師あり学習」を行い、広範な言語知識を獲得した後、特定のタスク（分類、抽出など）に合わせて少量のデータで**ファインチューニング（微調整）**を行うという二段階のプロセスが定着した。

#### 4. 大規模言語モデル（Large Language Models: LLM）

2020年の GPT-3 の登場以降、モデルのパラメータ数や学習データ量を劇的に増加させた（スケールアップした）段階を指す。

* **特徴**: 特定のタスクへのファインチューニングを必要とせず、プロンプトを通じた指示のみで多様なタスクを遂行する「コンテキスト学習（In-Context Learning）」や、複雑な推論を行う「創発的性質（Emergent Abilities）」が確認されるようになった。

現代の LLM は、単なる「次の単語の予測器」という定義を超え、膨大な統計的パターンの中に言語の構造、論理、そして世界知識を内包する高度な知能システムへと変貌を遂げている。次節では、この進化の鍵となった統計的アプローチからニューラルアプローチへのパラダイムシフトについて詳しく見ていく。

---

**次のステップとして、1.2 節（統計的言語モデルからニューラル言語モデルへ）の解説を作成しましょうか？**