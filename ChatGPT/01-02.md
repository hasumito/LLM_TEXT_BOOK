## 1.2. 「言語を扱う」とは何を意味するのか：確率・情報・意味

LLMが「言語を扱う」とは、端的には**文字列（あるいはトークン列）を確率分布としてモデル化し、その分布に基づいて次の要素を予測する**ことを意味する。ここで重要なのは、LLMが直接「意味」そのものを操作しているのではなく、観測可能な言語データの規則性を通じて、意味に強く結びついた構造を**確率的に近似**している点である。この節では、言語を確率として捉える視点、情報理論との関係、そして「意味」がどこに現れるのかを整理する。

### 言語を確率分布として捉える

自然言語の文は、単語や記号が順序をもって並んだ系列である。系列データを扱う基本の立場は、文を確率変数列 (X_1, X_2, \dots, X_T) とみなし、ある文 (x_{1:T}) の確率 (p(x_{1:T})) を定義することである。LLM（特にデコーダ型言語モデル）は、連鎖律に基づき

[
p(x_{1:T})=\prod_{t=1}^{T} p(x_t \mid x_{<t})
]

という分解を学習する。すなわち「これまでの文脈 (x_{<t}) が与えられたとき、次のトークン (x_t) が出現する条件付き確率」を近似する。生成とは、この条件付き分布からトークンを逐次的にサンプルし、系列を延長していく操作である。

この枠組みは一見すると「統計的な次単語当て」に過ぎない。しかし、言語が持つ構造—文法、語彙の選好、談話の整合、世界知識の反映—は、実データの中で条件付き確率として痕跡を残す。十分なデータと表現力を持つモデルは、その痕跡を圧縮して保持し、未観測の文脈に対してもそれらしい続きを高確率で出力するようになる。ここで扱っている対象は「意味」ではなく分布であるが、分布の形が意味と深く絡み合っている、というのが核心である。

### 「情報」としての言語：驚きと圧縮

確率が導入されると、情報理論が自然に接続される。ある事象 (x) の自己情報量（surprisal）は

[
I(x)=-\log p(x)
]

で与えられる。頻出する語や定型句は (p(x)) が大きく情報量が小さい。逆に、文脈にそぐわない語が突然出ると (p(x)) が小さく情報量が大きい。言語モデルは、文脈に対して「驚きが小さい」候補を選ぶ傾向を持つ。

系列全体の平均的な驚きはエントロピーや交差エントロピーに結びつく。真の分布を (P)、モデル分布を (Q) とすると、次トークン予測の学習は概ね

[
\mathbb{E}_{x\sim P}[-\log Q(x)]
]

を最小化することに相当する。これは「データを符号化するのに必要な平均ビット数を減らす」こと、すなわち**圧縮**と同型である。良い言語モデルとは、言語データをよく圧縮できるモデルであり、よく圧縮できるとは、言語の規則性（構文・語用・知識・文体など）を内部に取り込んでいることを意味する。LLMの学習目標は本質的に「次を当てる」だが、その裏面は「言語の持つ冗長性を削り、規則性を抽出する」作業である。

ただし注意すべき点がある。圧縮が上手いことは、必ずしも「真である内容」を保証しない。モデルが最小化しているのは「データ分布に対する符号長」であり、世界の事実そのものではない。したがって、学習データでしばしば観測されるが実際には誤っている言い回しや、もっともらしい虚構も、確率的に“あり得る”なら生成され得る。このズレが、後に扱うハルシネーション問題の地盤となる。

### 「意味」はどこにあるのか：分布が意味を代理する

人間が言語を理解するとき、「意味」は単語と世界の対応、命題の真偽、意図や含意などとして捉えられる。一方、LLMの形式化では、意味という語を直接の変数として置かないことが多い。それでもLLMが意味的に整合した出力を行うのは、言語使用の現実として、意味の違いが分布の違いに現れるからである。

この直観は分布仮説として知られる立場に近い。ある表現の意味は、その表現が現れる文脈分布（共起や周辺環境）によって特徴づけられる、という考えである。例えば「銀行（bank）」が金融機関なのか河岸なのかは、周辺に出る語（預金、金利、川、橋など）の分布が異なるため区別できる。LLMはこの種の分布差を大規模に学習し、文脈に応じて確率を割り当て直す。その結果として「意味の切り替え」に見える振る舞いが生じる。

しかし、この説明は「意味＝分布」と同一視するものではない。分布は意味を**代理**するが、代理は同一ではない。とくに以下の点が重要である。

* **指示対象（reference）の問題**：言語が指す現実世界の対象は、テキスト上の統計だけでは一意に定まらないことがある。
* **真理条件（truth-conditional semantics）の問題**：文が真か偽かは世界の状態に依存するが、学習はテキスト内の頻度に依存する。
* **語用論（pragmatics）の問題**：皮肉、婉曲、含意などは、状況・共同注意・目的を含む広い文脈に依存する。

LLMは、テキストから推測可能な範囲でこれらを近似しうるが、必ずしも外界の拘束を受けてはいない。この点で「意味を理解している」という表現は、慎重に使う必要がある。

### 生成とは「最もらしさ」の最適化である

LLMの出力は確率分布に基づくため、根本的に「最もらしい続きを作る」方向へ引力が働く。ここでの「最もらしい」は、学習分布に対しての最もらしさであり、必ずしも真実性、倫理性、有用性と一致しない。したがって、LLMを実用に供する際には、確率モデルとしての素の性質（尤度最大化）と、ユーザの期待（正確さ、根拠提示、安全性）の間にギャップが生じる。このギャップを埋めるために、後続章で扱うInstruction TuningやRLHF、さらにRAGやツール使用といった仕組みが導入される。

### 小結：確率・情報・意味の三角形

言語を扱うとは、(i) 言語列の確率分布を学習し、(ii) 情報量（驚き）を減らす方向に圧縮し、(iii) その圧縮によって意味と相関する構造を獲得すること、である。LLMは意味を直接計算する装置ではなく、分布を通じて意味に接近する装置である。ゆえに、強い流暢性と整合性が現れる一方で、真理や指示対象に関しては原理的な弱点も残る。この確率的理解を出発点として、次節では学習・推論・評価の全体像へ進む。


