## 2.3. 目的関数としての最尤推定と交差エントロピー

言語モデルは「次に出るトークンの確率分布」を出力する確率モデルである。したがって学習とは、観測されたテキスト列を最ももっともらしく生成できるように、モデルのパラメータ (\theta) を調整する問題として定式化できる。本節では、その目的関数が **最尤推定（Maximum Likelihood Estimation; MLE）** と **交差エントロピー（Cross-Entropy）** によって自然に導かれることを示す。

### 最尤推定としての言語モデル学習

まず、トークン列 (x_{1:T}=(x_1,\dots,x_T)) がデータ分布 (p_{\text{data}}) から生成されたとする。自己回帰言語モデルは連鎖律に基づき、モデル分布を

[
p_\theta(x_{1:T})=\prod_{t=1}^{T} p_\theta(x_t \mid x_{<t})
]

と因数分解して表す（ここで (x_{<t}=x_{1:t-1}) である）。学習データとして多数の系列（コーパス） (\mathcal{D}) が与えられたとき、最尤推定は「データがモデルの下で生起する確率（尤度）」を最大化する (\theta) を求める：

[
\theta^\ast=\arg\max_\theta \prod_{x\in\mathcal{D}} p_\theta(x)
]

計算の都合上、積の最大化は対数を取って和の最大化に変換するのが一般的である。系列 (x_{1:T}) に対する対数尤度は

[
\log p_\theta(x_{1:T})
=\sum_{t=1}^{T} \log p_\theta(x_t \mid x_{<t})
]

となる。よってコーパス全体の学習は、各時刻 (t) の「正解トークン (x_t) に割り当てた確率」の対数を足し合わせ、それを最大化することに等しい。逆に、通常の最適化（最小化）に合わせるため、負の対数尤度（Negative Log-Likelihood; NLL）を損失として最小化する：

[
\mathcal{L}*{\text{NLL}}(\theta)
= - \sum*{x\in\mathcal{D}} \sum_{t=1}^{T_x} \log p_\theta(x_t \mid x_{<t})
]

この形が、LLMの事前学習で用いられる「次トークン予測」の基本目的関数である。

### 交差エントロピーとしての解釈

上の損失は、情報理論の観点から **交差エントロピー** として解釈できる。離散確率分布 (p) と (q) の交差エントロピーは

[
H(p,q) = -\sum_{x} p(x)\log q(x)
]

で定義される。言語モデル学習では、各文脈 (x_{<t}) における「正解トークン」の分布は、教師信号として **one-hot 分布**（正解トークンに確率1、それ以外に0）として与えられる。語彙集合を (V) とし、正解が (y\in V) のとき、教師分布 (p_{\text{true}}(\cdot\mid x_{<t})) は

[
p_{\text{true}}(v\mid x_{<t})=
\begin{cases}
1 & (v=y)\
0 & (v\neq y)
\end{cases}
]

である。モデルが出力する分布を (p_\theta(\cdot\mid x_{<t})) とすると、交差エントロピー損失は

[
H!\left(p_{\text{true}}(\cdot\mid x_{<t}),,p_\theta(\cdot\mid x_{<t})\right)
= -\sum_{v\in V} p_{\text{true}}(v\mid x_{<t})\log p_\theta(v\mid x_{<t})
= -\log p_\theta(y\mid x_{<t})
]

となる。one-hot により和が一点に潰れるため、交差エントロピー最小化は「正解トークンの負の対数確率」を最小化することと一致する。したがって、系列全体・コーパス全体では、先のNLLと同一の目的関数が得られる。結論として、LLMの標準学習は **最尤推定（尤度最大化）** であり、同時に **交差エントロピー最小化** である。

### KLダイバージェンスとの関係

交差エントロピーは、教師分布とモデル分布の乖離を測る **KLダイバージェンス（Kullback–Leibler divergence）** と密接に関係する。一般に

[
H(p,q)=H(p)+D_{\mathrm{KL}}(p|q)
]

が成り立つ。ここで (H(p)) はエントロピー（分布 (p) 自身の不確実性）であり、(q) に依存しない定数である。したがって (H(p,q)) を最小化することは、(D_{\mathrm{KL}}(p|q)) を最小化することと等価である。言語モデル学習の文脈では、モデル分布 (p_\theta) がデータ分布 (p_{\text{data}}) に近づくように、KLの意味で整合させる操作と捉えられる。

ただし注意点がある。KLは非対称であり、(D_{\mathrm{KL}}(p_{\text{data}}|p_\theta)) を最小化する学習は、データが出現しうる領域に対してモデルが確率を割り当てることを強く要求する一方で、データで観測されにくい領域にモデルが余計な確率を割り当てることには比較的鈍感になりうる。この非対称性は、生成時に「それっぽいが誤った」出力が生じる背景理解にもつながる（この点は後の章で扱う）。

### 目的関数が“自然”である理由

最尤推定／交差エントロピーがLLMの目的関数として標準になっている理由は三つある。第一に、確率モデルとしての整合性が高いことだ。観測データの確率を最大化するという原理は、モデル化の最も基本的な立場である。第二に、自己回帰分解により、系列の学習が「次トークン分類問題」の和として実装でき、教師信号が明確で計算が安定することだ。第三に、対数尤度最大化は情報理論的には符号長最小化（最短記述長）とも結びつき、言語の予測可能性を直接的に最適化する点で目的が明瞭である。

以上より、LLMの事前学習は「次トークンの最尤推定」であり、その実体は「教師（one-hot）分布とモデル分布の交差エントロピー最小化」である、という理解が得られる。確信度：高
