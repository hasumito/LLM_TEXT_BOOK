## 1.1. LLM（大規模言語モデル）とは何か：できること／できないこと

LLM（Large Language Model; 大規模言語モデル）とは、**大量のテキスト（および場合によってはコード等）から、ある文脈に続く次のトークン（token）を高確率で予測するよう学習された確率モデル**である。ここでトークンとは、文字・単語・サブワード片などに分割された記号列の単位であり、LLMは入力文脈 (x_{1:t}) に対し次トークン (x_{t+1}) の条件付き分布 (p(x_{t+1}\mid x_{1:t})) を近似する。生成はこの分布からトークンを逐次サンプリング（あるいは最大確率選択）して進む自回帰（autoregressive）過程として記述できる。

現代のLLMの多くはTransformer系アーキテクチャに立脚し、自己注意（self-attention）を核として、長い文脈における依存関係を並列に取り扱う。学習は一般に、巨大なコーパス上での**次トークン予測**（最大尤度推定、実装上は交差エントロピー最小化）として定式化される。したがってLLMは「言語を理解しているか」という素朴な問いに対し、少なくとも数学的には「**分布を近似している**」と言うのが最も正確である。もっとも、その分布近似が結果として多様な言語タスクの性能を生む点が、LLMの実用的・研究的な核心である。

### できること（得意な能力の範囲）

LLMが得意とするのは、**言語的・記号的パターンの圧縮と再構成**である。次トークン予測という単純な目的でも、データ分布に含まれる多様な規則性が内部表現に埋め込まれ、以下のような能力として観測される。

* **テキスト生成一般**：要約、言い換え、翻訳、説明文生成、対話応答などを、文脈条件付き生成として統一的に扱える。
* **条件付き変換**：入力文を別の形式へ写像する（例：箇条書き化、スタイル変換、論点整理）。これは「入力→出力」の写像に見えるが、確率モデルとしては「入力を文脈に含めた次トークン生成」である。
* **弱い意味での推論・計画のように見える挙動**：多段の言語パターン（証明風の展開、手順列挙、因果関係の説明）を、訓練分布に頻出する形式として再現できる。ここで重要なのは、成功が「論理的正しさの保証」ではなく「高確率な言語的連鎖の生成」によって生じうる点である。
* **文脈内学習（in-context learning）**：追加学習なしに、プロンプト中の例示や指示に条件付けて振る舞いを変える。これはパラメータ更新ではなく、入力系列に対する条件付き分布の変化として理解される。
* **知識らしきものの再生**：学習データに含まれた事実・定義・手続きの多くを、適切な文脈で再構成できる。ただし、これは「外部世界の参照」ではなく「学習分布の反映」である。

要するに、LLMは「言語を入力すると言語で返す」装置というより、**文脈に整合的な記号列を高次元表現から復元する圧縮機械**として捉えるのが理論的に筋が良い。

### できないこと（原理的・統計的に生じる限界）

LLMの限界は、能力不足というより**目的関数と情報条件**からほぼ必然的に生じるものが多い。

* **真理性の保証ができない**：次トークン予測は「もっともらしさ」を最適化するが、「事実であること」や「検証可能性」を直接最適化しない。そのため、流暢だが誤った生成（ハルシネーション）が原理的に起こりうる。
* **外部世界への接地（grounding）がない**：単体のLLMはセンサーや実世界状態を持たず、生成は入力文脈に閉じる。したがって、最新情報・現場固有情報・観測を必要とする問いに対し、正答を保証できない。
* **不確実性の自己申告が不安定である**：確率分布を出力していても、「自分の答えが正しい確率」を人間が期待する形で校正（calibration）するのは別問題であり、モデルは自信過剰／過小になりうる。
* **長文脈での一貫性が崩れることがある**：有限のコンテキスト長と注意機構の制約により、長距離依存の保持や、文脈の中央部の情報参照が弱まる現象が起きうる（後章で扱う “Lost in the Middle” など）。
* **厳密な論理・数学・アルゴリズム実行の保証がない**：形式推論に見える出力は生成できるが、内部で確実な証明探索や計算手続きを逐次実行しているとは限らない。分布外の組合せ的一般化で破綻しやすい。
* **目的と評価のズレに弱い**：プロンプトの表現や順序のわずかな差で出力が大きく変化しうる。これは最適化対象が「人間の意図」ではなく「訓練分布上の尤度」であることの反映である。
* **バイアス・毒性・漏洩といったデータ由来の問題を免れない**：学習データの偏りが内部表現に残り、特定の属性・話題で偏向した応答が出たり、稀に学習データの断片が再生されるリスクがある（確率は低くともゼロではない）。

以上より、LLMは「知能」一般というより、**言語分布の強力な近似器**であり、正しさ・最新性・検証可能性・安全性といった要件は、モデル単体ではなくシステム設計（検証、検索、制約付与）で補う領域が大きい。

### 重要な補足：何が「できない」かは、モデル単体とシステムで変わる

ここで一つ反対方向の見方も述べる。LLMの限界として挙げた多くは、「LLMが不可能」というより「**LLM単体では保証できない**」が正確である。検索（RAG）、外部ツール、検証器、ルール、監査ログなどを統合すれば、最新性や事実性はかなり改善できる。しかし、その改善は「言語モデルが真理を理解した」ことを意味しない。**確率モデルの出力に、外部の根拠と制約を付けて信頼性を上げた**と解釈すべきである。この区別を誤ると、LLMの能力評価も、リスク評価も、必要な対策設計も一気に曇る。
