## 2.2. n-gram からニューラルLMへ：疎性と一般化

### n-gram言語モデルの基本形

言語モデルの目的は、ある語（トークン）列 (w_{1:T}) の確率を与えることである。連鎖律により
[
P(w_{1:T})=\prod_{t=1}^{T} P(w_t \mid w_{1:t-1})
]
と書けるが、履歴 (w_{1:t-1}) は長くなるほど組合せが爆発し、そのままでは推定不能になる。n-gramモデルはこれを**マルコフ過程**で近似し、直前 (n-1) 語だけに依存するとみなす：
[
P(w_t \mid w_{1:t-1}) \approx P(w_t \mid w_{t-n+1:t-1})
]
ここで (h=w_{t-n+1:t-1}) を履歴（history）とすると、最尤推定（MLE）は頻度比で与えられる：
[
\hat{P}(w\mid h)=\frac{c(h,w)}{c(h)}
]
この単純さがn-gramの強みであり、少量データでも「それらしい」分布を作れる一方、次節で述べる本質的な破綻点を抱える。

### 疎性（sparsity）：観測されないものが多すぎる問題

n-gramの中心的な困難は**疎性**である。語彙サイズを (V) とすると、取りうる履歴の数は概ね (V^{n-1}) に増える。現実のコーパス（観測データ）は有限であり、ほとんどの (n)-gramは**一度も観測されない**。するとMLEは容易に

* (c(h,w)=0 \Rightarrow \hat{P}(w\mid h)=0)
  を生み、未観測イベントに確率を割り当てられない。これは言語モデルとして致命的である。なぜなら、自然言語は創造的であり、未観測の組合せが日常的に現れるからである。

疎性は単に「データを増やせば解決する」問題ではない。データを増やしても、語彙の増加・表現の多様性・長い依存関係の存在により、未観測の穴は残り続ける。これは高次元の組合せが埋まりにくいという意味で、しばしば**次元の呪い（curse of dimensionality）**として理解される。

### スムージングとバックオフ：疎性への古典的対処

n-gramの発展は、疎性に対する確率推定の工夫の歴史でもある。代表的な考え方は次の二つである。

* **スムージング（smoothing）**：未観測の (n)-gramにも確率質量を与える（ゼロを避ける）。
* **バックオフ／補間（back-off / interpolation）**：高次の推定が信頼できないとき、低次（例：trigram→bigram→unigram）へ「退避」または混合する。

典型的には、割引（discounting）により観測済み事象の確率を少し削り、その分を未観測へ回す。高度な手法（例：Kneser–Ney系）では、単なる頻度ではなく「どれだけ多様な文脈で出現するか」を重視し、より妥当な一般化を実現する。これらは実務的に極めて有効であり、長らく強力なベースラインであり続けた。

しかし、ここには限界がある。スムージングは「ゼロ問題」を緩和するが、**意味的に近い語を近いものとして扱う**能力は基本的に持たない。n-gramは記号列の一致に依存するため、表記が一文字違えば別物としてカウントされ、知識の共有が起きにくい。

### 一般化（generalization）の壁：パラメータ共有がない

n-gramモデルは、履歴 (h) ごとに (P(\cdot\mid h)) をほぼ独立に学ぶ。これは統計学的には「状態（文脈）ごとに別のカテゴリ分布を推定する」ことに近い。すると、ある履歴で得た統計が別の履歴へほとんど移転しない。つまり**パラメータ共有（parameter sharing）が弱い**。

この結果として、

* 同義語・類義語・活用形・言い換えに対して脆い
* まれな履歴では推定が不安定
* (n) を上げるほど長距離依存は表現できるが、疎性が急激に悪化する
  というトレードオフが生じる。n-gramは「見たことがある断片」には強いが、「似ているが未観測」には弱いのである。

### ニューラル言語モデル：分散表現による滑らかな一般化

ニューラルLMは、この一般化の問題を**表現学習（representation learning）**で突破する。核心は次の一点にある。

> 単語を離散記号のまま扱わず、連続ベクトル（埋め込み）に写像し、その上で確率を計算する。

ニューラルLMは、履歴 (h) を何らかの関数 (f_\theta(h)) で連続表現 (z) に変換し、
[
P_\theta(w\mid h)=\text{Softmax}(g_\theta(z))_w
]
のように、パラメータ (\theta) を共有する形で全履歴・全語彙を同時に学習する。ここで重要なのは、**似た履歴は似た表現 (z) になりやすく、結果として似た予測分布を得やすい**という点である。これにより、n-gramが苦手だった「未観測だが妥当な組合せ」にも確率を割り当てられる。

歴史的には、固定長の文脈（例：直前数語）を入力して次語を予測するフィードフォワード型ニューラルLM（NNLM）が初期の代表例である。そこから、可変長の履歴を扱う再帰型（RNN系）へ進むが、この教科書では後章で系列モデリングとして整理する。

### 疎性への見方の転換：数え上げから関数近似へ

n-gramが「数え上げ（counting）」に基づく離散推定であるのに対し、ニューラルLMは「関数近似（function approximation）」である。疎性は、未観測事象の欠如というよりも、**どのような滑らかさ（inductive bias）で確率分布を補間するか**の問題に変換される。

* n-gram：一致した履歴にだけ強く依存（局所的・断片的）
* ニューラルLM：埋め込み空間で近いものを近く扱い、連続的に補間（滑らか・共有）

この差が、ニューラルLMの一般化性能を支える理屈である。

### 反対側からの見取り図：ニューラルが万能というわけではない

ここで一段冷静になる必要がある。ニューラルLMは一般化を改善するが、代償もある。

* **データと計算資源への依存**：表現学習は強力だが、十分なデータと最適化が前提になりやすい。
* **語彙外（OOV）問題の形の変化**：n-gramでは未知語は即座に破綻するが、ニューラルでもトークン化設計に強く依存し、未知の扱いは別の難しさとして残る。
* **解釈性の低下**：n-gramの確率は頻度に近く直感的だが、ニューラルLMの判断根拠は分散表現に埋め込まれ、透明性は下がる。

したがって、n-gram→ニューラルLMという流れは「古いものが新しいものに完全に駆逐された」という単純な物語ではない。疎性に対する解法が、**統計的工学（スムージング）**から**表現学習（パラメータ共有と滑らかさ）**へ主役交代した、と捉えるのが正確である。
