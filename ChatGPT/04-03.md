## 4.3. Seq2Seq と注意機構（attention）の登場

RNN（およびLSTM/GRU）は系列を扱えるが、入力系列から出力系列への変換（例：機械翻訳、要約、対話応答）のように、**入力長と出力長が異なり得る問題**を自然に定式化するには工夫が要る。この要求に対して確立した枠組みが **Seq2Seq（Sequence-to-Sequence）** である。Seq2Seqは、系列を別の系列へ写像するために、（1）入力系列を表現へ圧縮する **エンコーダ** と、（2）その表現を条件として出力系列を生成する **デコーダ** を組み合わせる構成である。

### Seq2Seq の確率モデルとしての定式化

Seq2Seqは本質的に条件付き言語モデルであり、入力系列 (x=(x_1,\dots,x_{|x|})) を条件に出力系列 (y=(y_1,\dots,y_{|y|})) の確率を
[
p(y\mid x)=\prod_{t=1}^{|y|} p(y_t \mid y_{<t}, x)
]
と自己回帰的に分解する。デコーダは「これまでに生成した出力 (y_{<t})」と「入力 (x) の情報」を使って次トークン (y_t) を予測する。学習は教師強制（teacher forcing）と呼ばれる形で、訓練時には (y_{<t}) としてモデルの生成結果ではなく正解系列の接頭辞を与え、交差エントロピー損失（負の対数尤度）を最小化する。

### エンコーダ–デコーダと「固定長ベクトルボトルネック」

初期のSeq2Seqでは、エンコーダRNNが入力を逐次読み込み、最終隠れ状態（あるいはその関数）を**文脈ベクトル** (c) としてデコーダへ渡す構成が一般的であった。デコーダRNNはこの (c) を条件に出力を生成する。直観的には、(c) は入力文全体を要約した固定長表現である。

しかしこの設計は、入力が長くなるほど情報を固定次元の (c) に押し込む必要があり、**固定長ベクトルの情報ボトルネック**が顕在化する。長文翻訳などで性能が劣化しやすいという経験的事実は、このボトルネックと整合的である。加えて、RNN系の学習はBPTTにより長距離依存が難しく、入力の初期部分の情報がデコーダへ届きにくいという問題も重なる。

### 注意機構（attention）の発想

注意機構は、この固定長ボトルネックを緩和するために導入された。要点は、デコーダが各時刻 (t) において「入力のどこを見るべきか」を動的に決め、固定ベクトル (c) の代わりに**時刻依存の文脈ベクトル** (c_t) を用いることである。

エンコーダは入力位置 (i) ごとに隠れ状態 (h_i) を出力する（これは入力系列の各位置に対応する表現の列である）。デコーダ時刻 (t) の状態を (s_t) とすると、注意は概念的に次の手順で定義される。

1. **整合度（スコア）**の計算
   [
   e_{t,i} = \text{score}(s_t, h_i)
   ]
   ここで (\text{score}(\cdot)) は「現在生成しようとしている状態 (s_t) と入力位置 (i) の表現 (h_i) がどれだけ関係するか」を測る関数である。内積、双線形形式、小さなニューラルネットなどが用いられる。

2. **重み（注意分布）**への正規化
   [
   \alpha_{t,i}=\frac{\exp(e_{t,i})}{\sum_j \exp(e_{t,j})}
   ]
   (\alpha_{t,i}) は時刻 (t) における入力位置 (i) への注意重みであり、入力全体にわたる確率分布として解釈できる。

3. **文脈ベクトル**の合成
   [
   c_t = \sum_i \alpha_{t,i} h_i
   ]
   すなわち、入力側の表現列 (h_i) の加重平均として、その時刻に必要な情報を抽出する。

デコーダは (c_t) と (s_t)（および過去の出力）を用いて (p(y_t\mid y_{<t}, x)) を計算する。これにより、モデルは生成の各段階で入力の異なる部分へ焦点を移すことができ、翻訳におけるアラインメント（対応付け）を内部的に学習する。

### 注意機構がもたらした意味

注意機構の導入により、Seq2Seqは「入力を一度固定長に圧縮してから生成する」方式から、「入力全体（の表現列）に対して必要に応じて参照する」方式へ移行した。これは、**情報の経路を短絡化する**という点で重要である。固定長ベクトルを介さずに、入力位置 (i) の情報が重み付き和として直接 (c_t) に寄与できるため、長い系列でも情報が失われにくい。さらに、注意重み (\alpha_{t,i}) は学習過程で分化し、系列変換タスクに必要な「どこを見て出力するか」という計算を顕在化させる。

もっとも、注意重みは確率分布の形を取るため直感的説明に使いやすい一方で、それ自体が因果的説明になっているとは限らない（注意が説明であるかは別問題）という点には留意が必要である。この論点は後の章で扱う。

### Transformerへの橋渡し

注意機構を備えたSeq2Seqは、RNNの逐次処理という制約を部分的に外し、「参照（addressing）」を学習可能な計算として前景化した。この流れの延長に、**RNNを捨て、注意を中核演算として全面化する**発想がある。すなわち、後に登場するTransformerは、Seq2Seq+attentionで得られた「入力表現列への動的参照」という利点を一般化し、自己注意（self-attention）としてモデル内部の各位置同士の相互参照へ拡張した体系である。Seq2Seqと注意機構は、TransformerとLLMへ至る系譜において決定的な転換点である。
