## 3.2. 分布仮説と共起：意味を「使われ方」で近似する

分散表現（embedding）の根底には、**分布仮説（distributional hypothesis）**がある。端的に言えば、**「ある語の意味は、その語がどのような文脈で使われるかによって特徴づけられる」**という立場である。古典的には「同じような文脈に現れる語は似た意味をもつ」と要約される。ここで重要なのは、意味を辞書的定義として固定せず、**観測可能な使用パターン（分布）**として近似する点である。

### 共起という観測量

分布仮説を計算可能にするために導入されるのが**共起（co-occurrence）**である。コーパス（大量のテキスト）を走査し、ある語 (w) の周辺（文脈）に別の語 (c) が現れた回数を数える。典型的には、対象語の左右 (k) 語を文脈窓（window）とし、そこで同時に出現した回数を

[
X_{w,c}=\text{count}(w \text{ co-occurs with } c)
]

として集計する。こうして得られる行列 (X) は**共起行列**であり、行（対象語）と列（文脈語）の双方が語彙で張られるため、高次元かつ疎（ほとんどがゼロ）になりやすい。だが、この疎行列に「意味の手がかり」が埋まっている、というのが分布仮説の具体的主張である。

### 「回数」だけでは意味にならない：確率化と情報量

単純な共起回数は頻度の高い語（例：「の」「は」「the」）に支配されやすい。そこで共起を確率として捉え直す。語 (w) と文脈 (c) の同時分布 (p(w,c))、周辺分布 (p(w),p(c)) を考えると、独立なら (p(w,c)=p(w)p(c)) である。独立からのずれを測る代表が**点相互情報量（PMI; pointwise mutual information）**である。

[
\text{PMI}(w,c)=\log\frac{p(w,c)}{p(w)p(c)}
]

PMI は「予想以上に一緒に出る」組を強調し、単なる頻度よりも意味的関連を拾いやすい。ただし PMI は低頻度事象で過大になりやすく、負の値（予想より一緒に出ない）も扱いづらい。実務では負の値を 0 に切る **PPMI（Positive PMI）** などがよく用いられる。ここでの本質は、意味を「共起の回数」ではなく、**共起の“意外性”＝情報量**として捉え直す点にある。

### 幾何としての意味：次元削減と低ランク構造

共起行列や PPMI 行列は巨大であるため、そのままでは扱いにくい。そこで「情報の圧縮」として**次元削減**を行う。代表例は **特異値分解（SVD）**などの低ランク近似である。行列を近似的に

[
X \approx U \Sigma V^\top
]

と分解し、上位の特異値成分だけを残すと、各語は低次元ベクトル（(U\Sigma) の行ベクトルなど）で表される。これは「共起パターンの主要な変動要因」を抽出する操作であり、結果として得られるベクトル空間では、**近さ（類似度）**が意味的近さの代理になる。類似度には **コサイン類似度**

[
\cos(\theta)=\frac{\mathbf{u}\cdot \mathbf{v}}{|\mathbf{u}||\mathbf{v}|}
]

がよく用いられる。長さ（頻度スケール）の影響を抑え、方向（共起パターン）を比較するためである。

### 予測モデルとしての共起：Word2Vec と目的関数

共起を「数える」立場（カウントベース）に対し、「当てる」立場（予測ベース）がある。代表が **Word2Vec**（CBOW / Skip-gram）である。たとえば Skip-gram は、対象語 (w) から周辺語 (c) を予測するモデルであり、

[
p(c|w)=\frac{\exp(\mathbf{v}_w^\top \mathbf{u}*c)}{\sum*{c'}\exp(\mathbf{v}*w^\top \mathbf{u}*{c'})}
]

のような形で学習する（(\mathbf{v}_w) は対象語ベクトル、(\mathbf{u}_c) は文脈側ベクトル）。巨大な分母は計算が重いため、**負例サンプリング（negative sampling）**などの近似が使われる。重要なのは、ここでも学習信号は結局「どの語とどの語が近くに現れたか」という共起構造から来ている点である。

さらに理論的には、予測ベース（特に負例サンプリング付き Skip-gram）は、ある条件下で **PMI（のシフト）を暗黙に分解している**と解釈できることが知られている。つまり、**カウントベースと予測ベースは対立というより、同じ共起情報を別形式で抽出している**とみなせる。

### 「意味＝使われ方」の強さと弱さ

分布仮説は強力だが万能ではない。ここで一段メタに、何ができて何が苦手かを押さえる。

* **多義性（polysemy）**：固定埋め込みは語ごとにベクトルが 1 つであるため、「bank（銀行／川岸）」のような多義を平均してしまう。これは次節の文脈化表現が解決を狙う中心課題である。
* **反義（antonymy）の近さ**：「暑い」と「寒い」のように反対語でも同じ話題・同じ文脈に出やすく、ベクトルが近づくことがある。分布仮説は「同じ文脈＝同じ意味」を保証しない。
* **機能語・頻度バイアス**：頻出語はあらゆる語と共起しやすく、単純な回数ベースでは意味がぼやける。PMI/PPMI、サブサンプリングなどはこの補正である。
* **窓幅の設計**：狭い窓は統語的（構文的）類似を拾いやすく、広い窓は話題的（意味的）類似を拾いやすい。何を「意味」と呼ぶかは設計に依存する。
* **分布外（domain shift）**：語の「使われ方」は領域で変わる。医療文書と SNS では同じ語の共起が変わり、埋め込みも変わる。これは後の章で扱うデータ分布の問題へ直結する。

ここで反対意見（別視点）も明示しておく。分布仮説は「意味を使用に還元」するため、直観的には過激である。実際、意味には指示対象（現実世界との対応）、身体性、社会的規範など、テキスト共起だけでは捉えにくい側面がある。したがって分布仮説は「意味そのものの定義」ではなく、**テキストから学習可能な意味の近似原理（作業仮説）**と捉えるのが適切である。それでもなお、この近似が驚くほど有効なのは、人間の言語活動が世界知識や推論をテキストへ大量に折り畳んでしまう、という事実に支えられている。

### LLMへの接続：共起から「予測」へ

LLM は入力トークン列から次トークンを予測するモデルであり、表面上は「共起行列」を作らない。しかし学習信号は、依然として **どのトークンがどの文脈で出たか**に由来する。すなわち、分布仮説と共起は「分散表現の起源」であると同時に、LLM の学習を理解するための最初の足場でもある。固定埋め込みは「語の平均的な使われ方」を幾何に写し、LLM の文脈化表現は「その場の使われ方」に応じてベクトルを動かす。次節では、この“固定から動的へ”の転換を扱う。
