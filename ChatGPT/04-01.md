## 4.1. RNNの基礎とBPTT（Backpropagation Through Time）

系列（sequence）データとは、入力が時間方向（あるいはトークン順）に並び、各要素が前後の文脈に依存しうるデータである。言語はその典型であり、次の単語（トークン）の確率は直前だけでなく、過去の履歴全体に依存する。RNN（Recurrent Neural Network）は、この「履歴への依存」を**再帰（recurrence）**によってモデル化する最も基本的な枠組みである。

### RNNの基本構造：隠れ状態による要約

RNNの中心概念は**隠れ状態（hidden state）** (h_t) である。時刻 (t) の入力 (x_t) と、1つ前の隠れ状態 (h_{t-1}) から、現在の隠れ状態 (h_t) を更新する。

代表的な（最も素朴な）RNNは次の形で書ける：

[
h_t = \phi(W_{xh}x_t + W_{hh}h_{t-1} + b_h)
]
[
y_t = g(W_{hy}h_t + b_y)
]

* (x_t)：時刻 (t) の入力（言語ならトークン埋め込みなど）
* (h_t)：過去の情報を要約する内部メモリ
* (y_t)：出力（言語モデルなら次トークン分布）
* (\phi)：非線形活性化関数（(\tanh) など）
* (g)：出力の変換（分類なら softmax）

このとき、同じ重み (W_{xh}, W_{hh}, W_{hy}) が全時刻で共有される点が重要である。すなわちRNNは「同一の更新規則を時間方向に繰り返し適用する機械」であり、系列長が変わっても同じパラメータで処理できる。

言語モデル（LM）の最も基本形は、自回帰分解
[
p(x_{1:T})=\prod_{t=1}^{T}p(x_t \mid x_{<t})
]
を近似することであり、RNNは (h_t) を (x_{<t}) の要約として用いて
[
p(x_{t+1}\mid x_{\le t})=\mathrm{softmax}(W_{hy}h_t)
]
のように条件付き分布を出力する。

### 計算グラフの「時間展開」とBPTTの発想

RNNの学習は勾配降下法に基づくが、RNNにはループ構造があるため、そのままでは通常の誤差逆伝播（backpropagation）を適用しづらい。ここで用いるのが **BPTT（Backpropagation Through Time）** である。

BPTTの要点は単純で、RNNの再帰を **時間方向に展開（unroll）**して、深いフィードフォワードネットワークとして扱うことである。系列長 (T) の入力に対し、

* (t=1,2,\dots,T) の各ステップが層のように並び、
* 各ステップのパラメータは共有される（同じ (W_{hh}) を何度も使う）

という形の計算グラフになる。損失が時刻ごとに定義されるとき（言語モデルなら次トークン予測の交差エントロピー）、全損失はたとえば

[
\mathcal{L} = \sum_{t=1}^{T} \ell_t
]

となる。BPTTでは、この展開グラフに通常の逆伝播を適用し、各時刻からの勾配を**共有パラメータに足し合わせる**。

直観的には、

* ある時刻 (t) の誤差は (h_t) を通じて過去へ伝播し、
* さらに (h_{t-1}), (h_{t-2}), … と連鎖的に影響を遡る
  という「時間方向の誤差伝播」を行う。

### BPTTで勾配が不安定になる理由：消失と爆発

RNNの歴史的な難点は、BPTTで長い系列を扱うと勾配が極端に小さくなる（消失）か、極端に大きくなる（爆発）ことである。これは数式上、勾配が繰り返しヤコビアン（微分行列）の積として現れることに由来する。

概念的に、ある過去の状態 (h_{t-k}) が現在の損失 (\ell_t) に与える影響は、更新写像の微分の積
[
\frac{\partial \ell_t}{\partial h_{t-k}}
;\propto;
\prod_{i=t-k+1}^{t}
\frac{\partial h_i}{\partial h_{i-1}}
]
のような形を含む。ここで各 (\frac{\partial h_i}{\partial h_{i-1}}) のスペクトル半径（おおざっぱに言えば“増幅率”）が

* 1より小さいと、積が指数的に0へ近づき勾配消失
* 1より大きいと、積が指数的に発散し勾配爆発
  が起きやすい。

この問題は「長期依存（long-range dependency）」、すなわち遠い過去の情報が必要なタスクほど深刻になる。言語では、主語—述語の対応や照応、話題の継続などが長期依存の例である。

### 実務上の対処：Truncated BPTTと勾配クリッピング

理想的には系列全体 (T) を展開してBPTTすべきであるが、計算量とメモリが線形に増えるため、現実には **Truncated BPTT（切り詰めBPTT）** が使われることが多い。これは時間方向に (K) ステップだけ遡って逆伝播し、それより前は勾配を流さない近似である。

* 計算・メモリ効率は上がる
* ただし、(K) より長い依存は学習しにくくなる（原理的な近似誤差）

また勾配爆発への代表的な対策として **勾配クリッピング（gradient clipping）** がある。勾配のノルムが閾値を超えたら一定値にスケールすることで、更新が破綻するのを防ぐ。

### RNNをどう評価すべきか：強みと限界

RNNは、系列を逐次処理し、隠れ状態に履歴を圧縮して運ぶという意味で、系列モデリングの出発点として美しい。一方で、ここまで述べた通り**長期依存の学習が不安定**であり、さらに逐次計算であるため**並列化が難しい**（時間方向に依存するためGPUでの効率が出にくい）。この二点は、のちにLSTM/GRU（4.2節）や注意機構（4.3節）、そしてTransformer（第5章以降）へと主役が移っていく決定的な理由になる。

ただし別の見方をすると、RNNは「状態を介して情報を圧縮し続ける」機構であり、有限メモリ機械としての性質を持つ。これは、注意機構のように過去へ直接アクセスする方式とは異なる帰納バイアス（学習しやすい仮定）を与える。したがってRNNは単なる過去の遺物ではなく、系列モデルの設計思想を理解するための基礎概念として重要である。

