## 4.2. LSTM/GRU：長期依存とゲート機構

RNNは系列データを逐次処理し、隠れ状態 (h_t) に過去の情報を要約して次の時刻へ渡す。しかし 4.1 節で述べた通り、BPTT において勾配が時間方向に連鎖的に伝播するため、ヤコビアンの積が指数的に縮退（勾配消失）または増幅（勾配爆発）しやすい。とりわけ勾配消失は「遠い過去の情報が学習信号として届かない」ことを意味し、長期依存（long-term dependency）の獲得を困難にする。LSTM（Long Short-Term Memory）と GRU（Gated Recurrent Unit）は、この問題を**ゲート機構（gating）**によって緩和するための代表的な設計である。両者の核心は、情報を「書き込む／保持する／消す」を連続値のゲートで制御し、時間方向の勾配経路により安定な“直通路”を用意する点にある。

---

### LSTM：セル状態による加法的メモリ

LSTMは隠れ状態 (h_t) に加えて、セル状態（cell state） (c_t) を導入する。通常のRNNが
[
h_t = \phi(W_x x_t + W_h h_{t-1} + b)
]
のように非線形変換で状態を更新するのに対し、LSTMはセル状態を主たる記憶媒体として、**加法的**に更新する。典型的な LSTM は以下で定義される（(\sigma) はシグモイド、(\odot) は要素ごとの積）：
[
\begin{aligned}
f_t &= \sigma(W_f x_t + U_f h_{t-1} + b_f) \quad \text{(忘却ゲート)}\
i_t &= \sigma(W_i x_t + U_i h_{t-1} + b_i) \quad \text{(入力ゲート)}\
\tilde{c}*t &= \tanh(W_c x_t + U_c h*{t-1} + b_c) \quad \text{(候補メモリ)}\
c_t &= f_t \odot c_{t-1} + i_t \odot \tilde{c}*t \quad \text{(セル状態更新)}\
o_t &= \sigma(W_o x_t + U_o h*{t-1} + b_o) \quad \text{(出力ゲート)}\
h_t &= o_t \odot \tanh(c_t) \quad \text{(隠れ状態)}
\end{aligned}
]

ここで重要なのは、セル状態 (c_t) の更新が **(c_{t-1}) の線形結合（加法）**を含む点である。忘却ゲート (f_t) が 1 に近い成分では (c_{t-1}) がほぼそのまま保持され、入力ゲート (i_t) が 0 に近い成分では新規情報の書き込みが抑制される。すなわち LSTM は、特徴次元ごとに「この情報は長く残す」「これは短期で上書きする」を学習的に決められる。

勾配の観点では、セル状態の時間方向の微分が概ね
[
\frac{\partial c_t}{\partial c_{t-1}} \approx f_t
]
となるため、(f_t) が 1 に近ければ勾配が減衰しにくい。通常のRNNが非線形の繰り返しにより勾配が縮退しやすいのに対し、LSTMは**「保持」を選んだ成分については勾配を通しやすい**構造を持つ。この“勾配の通り道”が、長期依存の学習を可能にする理屈である。

---

### GRU：簡略化されたゲート付き更新

GRUはLSTMに比べて構造を簡略化し、セル状態 (c_t) を明示的に分けず、隠れ状態 (h_t) の更新をゲートで制御する。代表的には更新ゲート (z_t) とリセットゲート (r_t) を用いて
[
\begin{aligned}
z_t &= \sigma(W_z x_t + U_z h_{t-1} + b_z) \quad \text{(更新ゲート)}\
r_t &= \sigma(W_r x_t + U_r h_{t-1} + b_r) \quad \text{(リセットゲート)}\
\tilde{h}*t &= \tanh(W_h x_t + U_h (r_t \odot h*{t-1}) + b_h) \quad \text{(候補状態)}\
h_t &= (1 - z_t)\odot h_{t-1} + z_t \odot \tilde{h}_t \quad \text{(状態更新)}
\end{aligned}
]
と定義される。

直観的には、更新ゲート (z_t) が「どれだけ新しい候補 (\tilde{h}*t) を採用するか」を決め、(1-z_t) が「どれだけ過去 (h*{t-1}) を保持するか」を決める。リセットゲート (r_t) は候補 (\tilde{h}*t) を作る際に過去状態をどれだけ参照するかを調整する。ここでも更新式が
[
h_t = (1-z_t)h*{t-1} + z_t \tilde{h}_t
]
という**加法的補間**になっている点が本質であり、保持成分では勾配が通りやすい。GRUはパラメータ数が相対的に少なく実装も簡潔で、タスクによってはLSTMと同等の性能を示すことがある。

---

### 「ゲート」とは何か：微分可能な条件分岐

LSTM/GRUにおけるゲートは、0/1の離散スイッチではなく、([0,1]) の連続値を取るシグモイド出力である。これにより「if 文のような分岐」を**微分可能**な形で実現している。結果としてモデルは、誤差逆伝播により「いつ忘れるべきか」「どの特徴を保持すべきか」を学習できる。ゲートは確率ではなく重みであるが、挙動としては「保持・更新の強度」を滑らかに調整するスカラー場として働く。

---

### 長期依存の意味と限界：万能ではない

LSTM/GRUは勾配消失を根本から消し去るのではなく、「保持が必要な情報に対しては勾配を通しやすくする」設計である。したがって、非常に長い系列（例えば数千〜数万ステップ）の依存を扱う場合には、依然として学習が難しいことがある。また逐次計算である以上、時刻方向に並列化しづらく、長文処理では計算効率の面でも不利になる。これらの事情が、後続の Seq2Seq と注意機構、さらに Transformer が登場する背景の一部となる（4.3 節につながる）。

それでもLSTM/GRUは、RNNが抱えた長期依存の弱点を「記憶の媒体」と「ゲート」によって工学的に克服し、系列モデリングの性能を大きく押し上げた重要な里程標である。長期依存を“構造で扱う”という発想は、Transformer時代の設計思想（情報の経路を意図的に作る）にも連続している。
