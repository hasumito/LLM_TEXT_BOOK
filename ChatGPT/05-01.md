## 5.1. Self-Attention：Query/Key/Value と重み付け

Self-Attention（自己注意）は、系列中の各トークンが「同じ系列内のどのトークンを、どれだけ参照して表現を更新するか」を学習的に決める機構である。Transformerにおいては、各位置の表現を、系列全体の情報を用いた**重み付き和**として再構成することで、長距離依存を含む多様な相互作用を一段で表現できる点が中核となる。

### Query/Key/Value の役割

系列長を (n)、各トークンの入力表現（埋め込みや前層の出力）を行列 (X\in\mathbb{R}^{n\times d_{\text{model}}}) とする。Self-Attentionでは、同じ (X) から3種類の表現を線形変換で生成する：

[
Q=XW_Q,\quad K=XW_K,\quad V=XW_V
]

ここで (W_Q\in\mathbb{R}^{d_{\text{model}}\times d_k})、(W_K\in\mathbb{R}^{d_{\text{model}}\times d_k})、(W_V\in\mathbb{R}^{d_{\text{model}}\times d_v}) は学習される重みである。直観的には、**Query（問い合わせ）**は「自分が今ほしい情報の型」、**Key（鍵）**は「各トークンが提供できる情報の索引」、**Value（値）**は「実際に集約される中身」である。重要なのは、参照先を決めるのに使う表現（Q,K）と、持ち帰る内容（V）を分離している点である。この分離により、「何に注目するか」と「そこから何を取り出すか」を別々に最適化できる。

### 類似度と重み付け：Scaled Dot-Product Attention

各位置 (i) のQueryベクトル (q_i) と、各位置 (j) のKeyベクトル (k_j) の内積 (q_i\cdot k_j) により類似度（整合度）を計算し、その値を正規化して重み（注意重み）に変換する。行列表現では次の形になる：

[
S=\frac{QK^\top}{\sqrt{d_k}}
]

(\sqrt{d_k}) によるスケーリングは、(d_k) が大きいと内積の分散が増え、softmaxが極端に飽和して勾配が小さくなりやすい問題を緩和するための設計である。次に、softmaxにより各行を確率分布として正規化し、注意重み (A) を得る：

[
A=\text{softmax}(S)
]

これにより、各位置 (i) は「系列中の各位置 (j) をどれだけ参照するか」という重み (A_{ij}) を持つ。最後に、Valueの重み付き和として出力を計算する：

[
\text{Attention}(Q,K,V)=AV
]

すなわち、位置 (i) の出力は (\sum_{j=1}^{n}A_{ij}v_j) であり、これは「系列全体からの情報の混合（mixing）」である。softmaxを用いるため、各行の重みは非負で総和が1となり、出力はValueの凸結合として解釈できる。なお、実装上は必要に応じて特定の位置を参照できないようにするマスク（例：パディング位置の無効化など）を (S) に加算してからsoftmaxを取ることが多いが、機構の本質は上式で尽くされる。

### Self-Attention がもたらす性質

第一に、各位置が系列全体へ直接アクセスできるため、RNNのように情報が逐次伝播する必要がなく、長距離依存を扱いやすい。第二に、注意重み (A) は入力に応じて変化するため、固定的な畳み込みカーネルのような局所ルールではなく、文脈依存の結合パターンを形成できる。第三に、計算は主に行列積 (QK^\top) と (AV) からなり、GPU等で並列化しやすい。

一方で、注意行列 (A) は (n\times n) であり、計算量・メモリ量が系列長 (n) に対して概ね二乗で増大するという制約を持つ。この点は長文処理におけるボトルネックとなり、後年の効率化研究（スパース化や近似など）へとつながる。

### 注意重みは「説明」か：別の見方

注意重み (A) は「どこを参照したか」を可視化できるため直観的であるが、これをそのままモデルの意思決定の説明と見なすのは慎重であるべきである。理由は少なくとも二つある。第一に、注意はValueを混合するための重みであり、最終出力は後段の線形変換や非線形、残差接続などを経て決まるため、重みが大きいことがそのまま因果的寄与を意味しない場合がある。第二に、同じ出力を与える異なる注意パターンが存在し得るため、注意は一意な説明ではない。したがって、Self-Attentionは「系列内依存を計算する機構」として理解するのが本筋であり、「解釈の窓」としては有用だが万能ではない、という立場が妥当である。

