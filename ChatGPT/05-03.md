## 5.3. 位置情報（Positional Encoding）：順序をどう埋め込むか

Transformer の自己注意（self-attention）は、入力系列を「集合（bag）」として扱う性質を持つ。すなわち、同じトークン集合であれば並び順を入れ替えても、注意計算そのものは原理的に区別できない。これは **置換不変（permutation invariant）** な構造であり、系列としての言語に必須な「順序」を失う。したがって Transformer では、**トークンの内容（embedding）とは別に、位置に関する情報をモデルへ注入する仕組み**が必要となる。これが位置情報（positional encoding / positional embedding）である。

### 位置情報を入れる基本設計

各位置 (t) のトークン埋め込みを (\mathbf{x}_t \in \mathbb{R}^d) とすると、最も基本的な方法は **加算**である：

[
\tilde{\mathbf{x}}_t = \mathbf{x}_t + \mathbf{p}_t
]

ここで (\mathbf{p}_t) が位置ベクトルである。加算を採用する理由は、次元 (d) を増やさずに位置情報を混ぜられ、以降の線形変換（(W_Q, W_K, W_V)）に自然に流し込めるからである。別案として連結（concatenation）もあるが、次元増加とパラメータ増を伴い、一般に計算・実装上の利点が薄い。

位置情報をどの段階に入れるかには流儀がある。入力埋め込みに加えるのが典型だが、**注意スコア（(\mathbf{Q}\mathbf{K}^\top)）側へ位置依存の項を足す**設計も広く使われる。後者は「相対位置」を扱いやすい。

---

### 絶対位置：位置 (t) をそのまま表現する

#### (1) 学習可能な位置埋め込み（Learned Absolute Positional Embedding）

位置ごとに学習可能なベクトル (\mathbf{p}_t) を用意し、訓練データを通じて最適化する方式である。実装が簡単で表現力も高い一方、**学習時に見た最大長を超える系列への外挿が弱い**という傾向がある。なぜなら、未知の位置 (t) に対応する (\mathbf{p}_t) が定義されない（あるいは補間が必要）からである。

#### (2) 正弦波位置エンコーディング（Sinusoidal Positional Encoding）

位置 (t) を決定論的な周期関数で符号化する方式である。代表例として、次元 (i) ごとに異なる周波数の (\sin,\cos) を割り当てる：

[
\mathbf{p}_t[2i] = \sin!\left(t / 10000^{2i/d}\right), \quad
\mathbf{p}_t[2i+1] = \cos!\left(t / 10000^{2i/d}\right)
]

この方式の狙いは、(i) パラメータを増やさない、(ii) 位置差が連続的に表現される、(iii) 原理的に任意長へ定義可能、の三点である。特に (iii) により **長さ外挿**に有利である、という直観が得られる。ただし「定義できる」ことと「うまく一般化する」ことは別であり、長文での性能は学習条件やアーキテクチャ全体（正規化、学習率、データ分布）にも依存する。

---

### 相対位置：距離 (t-s) を表現する（順序の本体はこちら寄りである）

言語において重要なのは「文頭から何番目か」よりも「どれだけ離れているか」「どちらが前か」である場合が多い。そこで、位置を **絶対値**としてではなく、**相対距離**として注意に反映する設計が生まれる。

自己注意の核であるスコアは

[
\mathrm{score}(t,s) = \frac{\mathbf{q}_t^\top \mathbf{k}_s}{\sqrt{d_k}}
]

である。相対位置法では、ここに距離依存のバイアス (b_{t-s}) を足す、あるいは (\mathbf{q}, \mathbf{k}) の生成に距離を組み込む：

[
\mathrm{score}(t,s) = \frac{\mathbf{q}_t^\top \mathbf{k}*s}{\sqrt{d_k}} + b*{t-s}
]

この形式は、**「どの語がどの語を見るべきか」を距離によって系統的に調整**できる。距離が近いトークンを優先する、句読点を越える注意を抑える、といった帰納バイアス（inductive bias）を入れやすい。

相対位置には多様な実装があるが、現代の LLM では特に次が重要である。

* **相対位置バイアス（Relative Position Bias）**：距離ごとにスカラー (b_\Delta)（(\Delta=t-s)）を学習し、注意スコアへ加える。距離のビン分割を併用することも多い。
* **RoPE（Rotary Positional Embedding）**：(\mathbf{q},\mathbf{k}) の各次元ペアを「回転」させることで相対位相（距離）を内積に埋め込む。概念的には、位置 (t) に応じた回転を (\mathbf{q}_t,\mathbf{k}_t) に適用し、(\mathbf{q}_t^\top \mathbf{k}_s) が **距離 (t-s)** の関数として振る舞うようにする方式である。加算型よりも「距離」を内積構造に自然に混ぜ込め、長文側の挙動が安定しやすいことが多い。
* **ALiBi（Attention with Linear Biases）**：注意スコアに距離に比例する負のバイアスを入れ、遠距離をなだらかに減衰させる。パラメータ増が小さく、長さ外挿の性質が比較的よい設計として知られる。

---

### 何が難しいのか：位置は「情報」だが「制約」でもある

位置情報は単に足せばよい付加情報ではなく、モデルの一般化を左右する制約条件でもある。ここで注意すべき論点は三つである。

1. **長さ外挿（length extrapolation）**
   学習時より長い系列で性能が落ちる現象は珍しくない。絶対位置の学習埋め込みは特に弱点が出やすい。相対位置系は原理的に外挿しやすいが、それでも学習データ分布が短文に偏ると長文での頑健性は保証されない。

2. **局所性と長距離依存のトレードオフ**
   言語は局所的な統語制約（近くを見る）と、長距離の照応・話題維持（遠くも見る）の両方を要求する。距離バイアスを強めすぎると長距離依存が壊れ、弱めすぎるとノイズ的注意が増える。位置設計は、モデルに「どれくらい遠くまで真面目に見るべきか」という暗黙の先験を与える。

3. **別の見方：位置は外部注入すべきか、内部で学ぶべきか**
   位置情報を明示的に注入するのは、人間が決めた表現形式を押し付けることでもある。対立する立場として、「位置はデータから学べるべきで、強い帰納バイアスは不要（あるいは有害）である」という考え方がある。ただし自己注意が置換不変である以上、**何らかの形で対称性を破らない限り順序は同定できない**。したがって実務的には、位置表現は「学習の自由度」と「必要最小限の制約」の妥協点として設計される。

---

### まとめ：位置情報は Transformer を「系列モデル」に戻すための鍵である

Transformer が言語を扱えるのは、自己注意が強力だからだけではない。**自己注意の置換不変性を、位置情報の注入によって意図的に破っている**からである。絶対位置（学習・正弦波）と相対位置（バイアス、RoPE、ALiBi など）は、いずれも「順序」を表現するが、**一般化（特に長文）と帰納バイアスの入れ方**が異なる。以降で扱う残差接続・正規化・FFNと同様に、位置表現も Transformer の実用性を決定づける部品であり、モデルの失敗様式（長文での破綻や注意の偏り）を理解する上でも中心的概念である。

