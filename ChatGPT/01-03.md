## 1.3. 学習・推論・評価の全体像（データ→学習→生成→運用）

LLMの仕組みを理解するうえで重要なのは、モデル単体（Transformerや損失関数）だけでなく、**「データから始まり、学習を経て、生成として現れ、運用で変質し、評価で戻される」**という循環系として捉えることである。本節では、LLM開発・提供の全体像を、理論的な観点から整理する。

---

### 1.3.1. 全体像は「確率モデル」を中心に回る

LLMは本質的に、トークン列 (x_{1:T}) に対して次トークン分布 (p_\theta(x_{t+1}\mid x_{\le t})) を与える**条件付き確率モデル**である。
ここで重要なのは、学習・推論・評価の全フェーズが、この確率分布 (p_\theta) を

* **どう学ぶか（学習）**
* **どう使うか（推論・生成）**
* **どの尺度で良し悪しを判定するか（評価）**
* **どの環境制約の下で安全に使わせるか（運用）**

という異なる角度から扱う点である。

---

### 1.3.2. データ：分布を決めるのは「何を食べたか」である

学習はデータ分布 (p_{\text{data}}(x)) に対して行われるため、モデルが獲得する性質は（理想化すれば）**データ分布の写像**である。したがってデータ工程は単なる前処理ではなく、LLMの能力と限界を規定する中心工程である。

* **収集（collection）**：Web、書籍、論文、コード、対話ログなどの混合分布を形成する。
* **フィルタリング（filtering）**：ノイズ除去、有害性低減、重複排除、言語判定などで分布を「整形」する。
* **正規化（normalization）**：文字種統一、改行規則、メタデータ処理などで表現のばらつきを抑える。
* **トークン化（tokenization）**：BPE/Unigram等により、連続文字列を離散記号列へ写像する。ここで語彙設計が学習効率・表現力・多言語性に影響する。

注意すべき点として、データ工程には**評価データへの混入（データ汚染）**という構造的リスクがある。これは「評価が高い＝汎化した」とは限らない原因になり、LLM評価を難しくする。

---

### 1.3.3. 学習：目的関数を最小化して「次トークン予測器」を作る

学習の中心は、パラメータ (\theta) を最適化して、観測されたトークン列の尤度を高めることである。典型的には**交差エントロピー損失**（負の対数尤度）を最小化する。

[
\mathcal{L}(\theta)= -\sum_{t}\log p_\theta(x_{t+1}\mid x_{\le t})
]

この枠組みには2つの重要な含意がある。

1. **学習目標は「次トークンの当てっこ」であり、真理判定器ではない。**
   整合性・一貫性・正確性は、データ分布とモデル容量が許す範囲での副産物である。

2. **最適化は確率分布の近似であり、知識の格納は結果である。**
   モデルは「事実のデータベース」ではなく、「データに現れた規則性の圧縮表現」を内部に形成する。

また実務上は、事前学習（pretraining）に加えて、指示追従のためのSFT（教師あり微調整）や、選好に基づく最適化（RLHF等）によって、同じモデルでも生成分布の性質が変わる。ここで起きているのは、確率モデルの**目的関数の付け替え**であり、「何を良い出力とみなすか」の定義変更である。

---

### 1.3.4. 推論・生成：同じ確率分布でも「取り出し方」で性格が変わる

推論（inference）は、学習済み (p_\theta(x_{t+1}\mid x_{\le t})) を用いてトークンを逐次生成する過程である。重要なのは、生成は確率分布からの**サンプリング**（あるいは近似的探索）であり、決定論ではない点である。

* **貪欲法（greedy）**：各時刻で最大確率トークンを選ぶ。安定だが多様性が低い。
* **ビームサーチ（beam search）**：複数候補を保持して探索する。翻訳などで用いられたが、長文対話では必ずしも最良とは限らない。
* **温度（temperature）**：分布の尖り具合を調整し、創造性と安定性をトレードオフする。
* **トップk・トップp（nucleus）**：候補集合を制限して、低確率ノイズを切る。

同一のモデルでも、デコーディング規則が違えば出力の性格が変わる。したがって「モデルが賢い／賢くない」という議論は、しばしば**生成アルゴリズムと運用設定の影響**を混同している。

---

### 1.3.5. 運用：モデルは「環境」と結合してはじめて製品になる

運用では、LLMは単体で置かれず、通常は以下と結合される。

* **システムプロンプト／ポリシー**：振る舞いの上位制約。
* **ツール使用（関数呼び出し等）**：外部計算・検索・実行系と接続し、能力を拡張する。
* **RAG（検索拡張生成）**：外部コーパスから取得した文書を条件として与え、事実性・最新性を補う。
* **キャッシュ、レート制限、監査ログ**：性能・コスト・ガバナンス要件を満たす。

ここでの要点は、運用により実際にユーザが触る対象は
[
\text{LLM製品} \approx (\text{モデル}) + (\text{プロンプト}) + (\text{検索}) + (\text{ツール}) + (\text{制御})
]
という**複合システム**になることである。ゆえに、問題が起きたときの原因は「モデルが悪い」とは限らず、検索、プロンプト、ツール設計、データ鮮度、UI誘導など多層に分布する。

---

### 1.3.6. 評価：学習の指標と、製品の指標は一致しない

評価は大別して次の二系統に分かれる。

* **内的評価（学習指標）**：損失、パープレキシティなど。モデルが訓練分布をどれだけよく近似したかを測る。
* **外的評価（タスク指標）**：QA、要約、推論、対話品質、安全性、毒性、幻覚率など。実用上の価値やリスクを測る。

ここで重要な原則は、**交差エントロピー損失の改善が、必ずしもタスク品質の改善を保証しない**ことである。理由は複数あるが、代表的には以下である。

* 評価タスクが訓練分布とズレている（分布シフト）。
* 生成はサンプリングであり、損失は平均的な確率近似であって「望ましい1出力」を保証しない。
* 人間の好みや安全性は、次トークン予測の自然な帰結ではなく、追加の制約である。

さらに、評価は運用後も続く。モデル更新、プロンプト変更、外部知識ベース更新により振る舞いが変わるため、評価は一度きりの試験ではなく、**継続的監視（monitoring）**を含む。

---

### 1.3.7. フィードバックループ：全体は循環し、固定されない

LLMの現実のライフサイクルは直線ではなく循環である。

1. **データ**が分布を定める
2. **学習**が分布を近似する
3. **生成**が確率分布から出力を生む
4. **運用**が環境との結合で振る舞いを変える
5. **評価**が欠陥と要件を発見する
6. 発見が再び **データ設計・学習設計**に戻る

このループの中で特に厄介なのは、運用で生まれたログ（ユーザ入力やモデル出力）を再学習に用いると、分布が自己参照的に歪みうる点である。これはモデルの自己増幅や品質劣化（あるいは安全側への過剰収束）を招き得るため、「データに何を入れるか」は運用後も慎重に設計されるべき対象となる。

---

### 1.3.8. 別の視点：LLMは「モデル」ではなく「最適化されたインターフェース」でもある

ここまでの説明は確率モデル中心であるが、別の見方も有用である。LLMを「人間が自然言語で計算資源を呼び出すためのインターフェース」とみなす立場である。この見方では、

* 学習は「言語的指示を計算手続きへ写像する能力」を獲得する工程
* 推論は「状況（コンテキスト）に合わせて手続きを実行する」工程
* 評価は「期待する手続きの作動確認」
* 運用は「失敗時の安全装置と責任分界の設計」

として再解釈される。どちらの枠組みも真であり、前者は理論理解に、後者はシステム設計に効く。重要なのは、視点を固定して単純化しすぎないことである。

