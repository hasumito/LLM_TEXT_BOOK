## 2.1. 言語の確率モデル：連鎖律と条件付き確率

言語モデル（Language Model, LM）とは、文字列（トークン列）として表現された言語データに対し、その出現確率を与える確率モデルである。LLM（大規模言語モデル）が行う生成は、見かけ上は「文章を書く」振る舞いであるが、理論的には「次に現れるトークンの条件付き確率分布を逐次的に与える」計算として定式化される。本節では、その基礎となる連鎖律（chain rule）と条件付き確率を整理し、言語を確率として扱う最小単位の枠組みを確立する。

### 言語を確率変数として見る

文をトークン列 (x_{1:T}=(x_1,x_2,\dots,x_T)) と表す。ここでトークンとは、単語・サブワード・文字など、モデルが扱う離散記号の単位である（トークン化の詳細は後節で扱う）。言語の確率モデルは、このトークン列全体の同時確率
[
p(x_{1:T})
]
を定義する。直観的には「その文が現れるもっともらしさ」を数値として与えるものである。

ただし、自然言語は長さ (T) が可変であり、さらに語彙サイズ (|\mathcal{V}|) が大きい。そのため、同時確率を直接テーブルとして持つことは不可能である。そこで、確率の連鎖律により同時確率を条件付き確率の積に分解し、局所的な予測問題として扱う。

### 条件付き確率と連鎖律

確率の基本として、条件付き確率は
[
p(a\mid b)=\frac{p(a,b)}{p(b)}
]
で定義される（(p(b)>0) を仮定する）。この定義から、二変数の同時確率は
[
p(a,b)=p(a\mid b),p(b)
]
と書き換えられる。これを多変数へ一般化したものが連鎖律である。トークン列 (x_{1:T}) に対して、
[
p(x_{1:T}) = \prod_{t=1}^{T} p(x_t \mid x_{1:t-1})
]
が成り立つ。ここで (x_{1:0}) は空の履歴であり、(p(x_1\mid x_{1:0})) は文頭トークン（あるいは開始記号）からの確率として解釈するのが一般的である。

この分解は「同時確率を逐次予測の積として表現できる」ことを意味する。すなわち、言語モデルを構築するとは、各時刻 (t) において履歴 (x_{1:t-1}) が与えられたときの次トークン分布
[
p(\cdot \mid x_{1:t-1})
]
を与える関数を学習することに等しい。

### 自回帰モデルとしての言語モデル

連鎖律による分解に基づく言語モデルは、自回帰（autoregressive）モデルと呼ばれる。自回帰とは「過去の観測（ここでは過去トークン）に条件付けて次を生成する」形式であり、生成過程は次のように記述できる。

1. 文頭（開始記号）を与える
2. (p(x_1)) または (p(x_1\mid \text{BOS})) から (x_1) を得る
3. (p(x_2\mid x_1)) から (x_2) を得る
4. 一般に (p(x_t\mid x_{1:t-1})) から (x_t) を得る
5. 終了記号（EOS）が出るまで繰り返す

このように、LLMの生成は本質的に「連鎖律で分解された条件付き確率を用いた逐次サンプリング（あるいは最大確率選択）」である。見かけ上の流暢さや一貫性は、各ステップでの条件付き分布が言語の統計的規則性をうまく捉えていることから生じる。

### 「条件付き」が意味するもの：文脈依存性

条件付き確率 (p(x_t\mid x_{1:t-1})) は、次トークンの分布が履歴に依存して変化することを表す。例えば同じ語でも、直前の語や文全体の流れにより適切さが変わる。言語理解・生成における「文脈（context）」とは、まさに条件付けに使われる情報 (x_{1:t-1}) を指す。この枠組みにより、曖昧性解消や共参照（代名詞が何を指すか）など、多くの言語現象が「履歴条件付きの予測」として一つの形に統合される。

一方で、この条件付けは理想的には「過去すべて」に依存するが、実装上は計算資源やモデル設計の制約を受ける。後の章で扱うように、n-gramは有限長の履歴に切り詰め、Transformerは有限長コンテキスト内で注意機構により依存関係を表現する。いずれにせよ、言語モデルの中心課題は「どのように (p(x_t\mid x_{1:t-1})) を表現し、推定するか」である。

### 小括：本節の位置付け

連鎖律は確率論の恒等式であり、近似でも仮定でもない。したがって、言語モデル研究の多くは「連鎖律で分解した条件付き確率を、どのモデルクラスで、どの目的関数で、どのように推定するか」という設計問題として理解できる。本節で導入した分解
[
p(x_{1:T}) = \prod_{t=1}^{T} p(x_t \mid x_{1:t-1})
]
は、以降の章で扱う n-gram、ニューラルLM、Transformer、そしてLLMの学習・生成・評価を貫く背骨となる。
