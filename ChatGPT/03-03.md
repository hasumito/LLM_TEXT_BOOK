## 3.3. 文脈化表現への道：固定表現から動的表現へ

3.1節で述べた埋め込み（embedding）および3.2節の分布仮説に基づく分散表現は、語彙項目（トークン）ごとに**1つのベクトル**を割り当てる「固定表現」である。これは計算上扱いやすく、多くの下流タスクで有効であった一方、言語の本質的な性質――**同じ語が文脈により異なる意味・機能を持つ**――を表現するには限界がある。ここから、同一トークンであっても入力文脈に応じて表現が変化する「文脈化表現（contextualized representation）」へと視点が移る。

### 固定表現の限界：多義性・同形異義・機能語

固定表現の弱点は、語の意味が単一の点に潰れることに起因する。典型例として「bank」は金融機関と河岸の両方を指すが、固定表現では両義を平均したようなベクトルになりやすい。結果として、周辺語の違い（“money” と “river”）を手がかりに意味を切り替える必要があるタスクで、表現が情報不足となる。同様に、日本語の同形異義語や、品詞・統語機能が文脈で変わる語（例：英語の “can” の助動詞用法と名詞用法）も、固定表現では区別しにくい。
さらに、固定表現は語彙項目の意味に寄与しやすい一方で、**文の構造**（主語・目的語・修飾関係）や**長距離依存**（離れた語の照応、否定の作用域など）を直接的に符号化することが難しい。これらは語の「同時出現」だけでなく、語同士の関係の向きや階層（統語）に強く依存するためである。

### 分布から文脈へ：表現の単位を「語」から「出現」に変える

文脈化表現の核心は、表現の対象を「語タイプ（type）」ではなく「語の出現（token）」へ移す点にある。すなわち、同じ語彙項目 (w) であっても、文脈 (c) によって表現 (\mathbf{h}(w,c)) が変化する。
ここで重要なのは、分布仮説の精神が捨てられるわけではないことである。むしろ「意味は使われ方で近似できる」という考えを、より直接に実装する。固定表現が「コーパス全体での平均的な使われ方」を圧縮するのに対し、文脈化表現は「この文、この位置での使われ方」を圧縮する。したがって、同じ語が異なる周辺語・構文・談話条件の下で現れるとき、表現も異なる点へ写像されることになる。

### 逐次モデルと隠れ状態：動的表現の原型

文脈化表現の初期の自然な器は、系列モデリングである。RNN（再帰型ニューラルネットワーク）やLSTM/GRUでは、時刻 (t) の隠れ状態 (\mathbf{h}_t) が、直前までの入力列 ((x_1,\dots,x_t)) の関数として更新される。これにより、各トークン位置に**文脈依存の表現** (\mathbf{h}_t) が得られる。双方向RNN（BiRNN）を用いれば、左文脈と右文脈の双方を統合した表現も得られる。
ただし、これらの逐次モデルでは情報が単一の状態に逐次的に圧縮されるため、長距離の依存関係や並列計算の効率に課題が残る。これが後の注意機構およびTransformerへと繋がる動機となる（第4章以降で扱う）。

### 文脈化表現の学習：タスク依存の特徴抽出から汎用表現へ

文脈化表現が本格的に意味を持つのは、「表現を作るモデル」自体が大規模データで学習され、下流タスクへ転移できるようになってからである。ここでの鍵は、表現学習を単一タスクのための特徴抽出としてではなく、**汎用的な言語表現の獲得**として位置づける点にある。
学習目標としては、言語モデル（次トークン予測）やマスク言語モデルなどの自己教師あり学習が典型である。これらは明示的なラベルを要せず、コーパスそのものから学習信号を生成できる。結果として、モデルは統語・語義・共参照・談話といった多層の規則性を内在化しやすくなる。重要なのは、文脈化表現が「意味辞書」を内部に持つのではなく、**予測に役立つ統計的規則性**を多段の表現変換として獲得する点である。

### 文脈化の含意：意味は一点ではなく、条件付き分布である

固定表現から文脈化表現への移行は、「語の意味」をベクトル一点として捉える見方から、「文脈条件付きで変化する対象」として捉える見方への転換である。言い換えると、意味とは静的な属性ではなく、入力条件に依存して立ち上がる機能的な量である。
この観点は、LLMの理解にも直結する。LLMはトークン列に対して各位置の表現を段階的に更新し、最終的に次トークン分布を出力する。そこで内部表現は、固定的な辞書埋め込みから出発しつつも、層を重ねるほど文脈依存に洗練される。したがって、文脈化表現は「分散表現の発展形」であると同時に、以後のTransformerにおける自己注意が担う表現形成の土台でもある。
