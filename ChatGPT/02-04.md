## 2.4. パープレキシティとその解釈

パープレキシティ（perplexity; PPL）は、言語モデルが「次のトークンをどれだけ迷わず当てられるか」を要約する代表的な評価指標である。直観的には、モデルが各時刻で提示される候補をどれほど広く見積もっているか、すなわち平均的に何択ぐらいで迷っているかを表す量と理解できる。ただし、この直観は一定の仮定の下で成立するため、解釈には注意が必要である。

### 2.4.1. 交差エントロピーからの定義

前節で扱った通り、言語モデルは系列 (x_{1:T}) に対して
[
p_\theta(x_{1:T})=\prod_{t=1}^{T} p_\theta(x_t\mid x_{<t})
]
で確率を与える。評価データに対する平均負の対数尤度（=平均交差エントロピー、単位は「nats」）を
[
H_{\text{nats}} ;=; -\frac{1}{T}\sum_{t=1}^{T}\log p_\theta(x_t\mid x_{<t})
]
と定義すると、パープレキシティはその指数として
[
\mathrm{PPL} ;=; \exp!\left(H_{\text{nats}}\right)
]
で与えられる。対数の底を 2 にした平均交差エントロピー（単位は「bits」）
[
H_{\text{bits}} ;=; -\frac{1}{T}\sum_{t=1}^{T}\log_2 p_\theta(x_t\mid x_{<t})
]
を用いる場合は
[
\mathrm{PPL} ;=; 2^{H_{\text{bits}}}
]
となる。つまり、**パープレキシティは「平均交差エントロピーを元の確率スケールに戻したもの」**であり、損失の単調変換にすぎない。したがって、パープレキシティを下げることと交差エントロピーを下げることは同値である。

### 2.4.2. 「平均何択か」という直観の由来

(\mathrm{PPL}) は幾何平均としても書ける。上式より
[
\mathrm{PPL}
= \exp!\left(-\frac{1}{T}\sum_{t}\log p_\theta(x_t\mid x_{<t})\right)
= \left(\prod_{t=1}^{T}\frac{1}{p_\theta(x_t\mid x_{<t})}\right)^{!!1/T}.
]
すなわち、各時刻での「正解トークンの逆確率」 (\frac{1}{p_\theta(\cdot)}) の幾何平均である。もしモデルが毎ステップで常に一様に (K) 個の候補に確率を割り当て、正解に (1/K) を与えるような理想化状況なら、平均損失は (\log K) となり、(\mathrm{PPL}=K) となる。ここから「平均 (K) 択」という解釈が生まれる。

しかし現実の言語では、分布は一様でも固定でもない。文脈によって次トークンの不確実性は大きく変動し、語彙の頻度偏りも強い。ゆえにパープレキシティは、**不確実性の変動をならした“平均的な困惑度”**であり、逐語的に「何択問題」を解いているわけではない。

### 2.4.3. パープレキシティの比較に潜む落とし穴

パープレキシティは便利だが、比較の条件が少しでもずれると数値が無意味になりやすい。主な注意点を述べる。

**(1) トークン化（語彙分割）依存**
PPL は「トークン列」に対する量であるため、BPE や Unigram などのトークナイザが異なれば、同じ文章でも (T) や条件付き確率の単位が変わる。したがって、**異なるトークン化を用いるモデル間で PPL を単純比較してはならない**。比較は原則として「同一トークナイザ・同一前処理・同一評価集合」で行う必要がある。

**(2) 評価データ分布依存**
PPL は評価集合の分布に強く依存する。新聞記事で低いモデルが、SNS やコードで低いとは限らない。ゆえに PPL は「汎用的な賢さ」ではなく、**その分布での次トークン予測性能**を測る指標である。

**(3) 長さ正規化と希少事象の影響**
平均対数確率は、少数の極端に低い確率（固有名詞、数値、表記揺れ、未知語の分割など）に大きく引きずられる。言い換えると、PPL は「モデルが外した箇所」に敏感である。これは言語モデリング評価としては自然だが、下流タスクの品質とは必ずしも一致しない。

**(4) “確率の良さ”と“生成の良さ”は同義ではない**
PPL が低いことは、真の分布に近い確率割当てができていることを意味するが、それだけで対話品質や有用性が保証されるわけではない。生成ではデコード戦略（温度、トップ (p) など）や安全性制約、指示追従のための微調整が介在する。これらは確率モデルを変形し得るため、**PPL と人間評価が乖離する**ことは珍しくない。

### 2.4.4. 情報理論的な解釈：圧縮としての言語モデル

パープレキシティは情報理論とも直結している。(H_{\text{bits}}) は「1トークン当たり平均何ビットで符号化できるか」に対応し、より低い値はより良い圧縮を意味する。言語モデルを確率符号（例：算術符号）に用いると、理想的には (-\log_2 p_\theta(x_t\mid x_{<t})) ビットでそのトークンを符号化できるため、平均は (H_{\text{bits}}) に近づく。ゆえに、PPL は
[
\mathrm{PPL} = 2^{H_{\text{bits}}}
]
として「平均符号長（bits/token）を指数化したもの」であり、**モデルがどれだけ“言語の統計的規則性”を捉えたか**の尺度と見なせる。

ただしここでも、トークン化が変われば「token」の単位が変わり、bits/token の意味も変わる。より普遍的な比較には、文字単位やバイト単位への正規化（bits/char, bits/byte）の発想があるが、実務上は同一設定での相対比較が主となる。

### 2.4.5. まとめ：何を言えて、何を言えないか

パープレキシティは、言語モデルの学習目的（交差エントロピー）と整合した、計算しやすく再現性の高い指標である。一方で、それは **「特定のトークン化・特定の分布上での次トークン予測の平均性能」**に過ぎず、対話能力、推論能力、指示追従、事実性、安全性といった側面を直接測るものではない。したがって、PPL は言語モデリングの基本性能を把握する土台として有用であるが、モデルの実運用価値を論じるには、下流タスク評価や人手評価、頑健性評価などと併用すべきである。


