## 3.1. one-hot表現の限界と埋め込み（embedding）

言語モデルが文字列を扱うためには、離散的な記号（単語・サブワード・文字など）を、ニューラルネットが処理できる連続値ベクトルへ写像する必要がある。このとき最も素朴な表現が **one-hot 表現**である。語彙サイズを (V) とすると、各トークンは (V) 次元のベクトルで表され、該当するトークンの次元のみが 1、それ以外は 0 となる。one-hot は「トークンの同一性」を完全に保つという意味では正確であるが、学習と一般化の観点から重大な限界を持つ。

第一の限界は、**幾何構造を持たない**ことである。one-hot ベクトル同士の内積は、同一トークンなら 1、異なるトークンなら 0 であり、異なるトークン同士はすべて等距離（ユークリッド距離なら (\sqrt{2})）に配置される。したがって「犬」と「猫」は「犬」と「電子レンジ」と同程度に無関係として扱われ、意味的な近さや類似性が表現に反映されない。言語の本質である「似た使われ方の語は似た意味を持つ」という性質を、one-hot は幾何として表現できないのである。

第二の限界は、**高次元かつ疎で非効率**である点である。語彙が数万〜数十万規模になると、one-hot は次元が極めて大きくなる一方で非ゼロ要素は 1 つしかない。この疎性自体は計算実装で回避できることもあるが、より本質的には「入力表現が学習すべき類似性を一切含まない」ため、モデルは上流の表現層でそれを獲得し直さなければならない。つまり one-hot は、学習の出発点として情報が乏しすぎる。

第三の限界は、**統計的強度（statistical strength）の共有が起きない**ことである。言語にはデータ頻度の偏りがあり、レアな語は観測回数が少ない。one-hot は語ごとに独立な軸を持つため、レア語のパラメータ推定において頻出語からの知識転移が起こりにくい。結果として、頻度の低いトークンほど表現が不安定になり、一般化性能が落ちる傾向が生じる。

これらの限界を克服する基本的手段が **埋め込み（embedding）**である。埋め込みとは、語彙 ({1,\dots,V}) の各トークン (i) に対し、低次元の連続ベクトル (\mathbf{e}_i \in \mathbb{R}^d)（通常 (d \ll V)）を割り当てる写像である。実装としては、埋め込み行列 (E \in \mathbb{R}^{V \times d}) を学習し、トークン (i) の埋め込みを (E) の (i) 行（あるいは列の取り方は流儀による）として取り出す。one-hot ベクトル (\mathbf{x}_i) を用いるなら (\mathbf{e}_i = \mathbf{x}_i^\top E) と書け、one-hot に対する線形変換の一種として理解できる。ただし計算上は行参照（lookup）として実装されるのが一般的である。

埋め込みが導入する最大の変化は、トークン集合に **学習可能な幾何**を与えることである。ベクトル空間上で内積やコサイン類似度が意味を持ち、使用文脈が似た語は近く、異なる語は遠く配置されうる。こうして「意味の近さ」を連続空間の近さとして表現できるようになる。さらに、低次元化により表現が密になり、学習は「無関係な軸の集合」ではなく「共有された潜在特徴の組み合わせ」として進む。これは統計的強度の共有を促進し、レア語にも周辺語との関係を通じて情報が伝播しやすくなる。

埋め込みはまた、ニューラル言語モデルの最初期から現在に至るまで一貫して中心的部品である。Transformer 系モデルでは、入力トークン埋め込みが自己注意層へ渡され、層を重ねることで固定埋め込みから文脈化表現へと変換されていく。重要なのは、埋め込みは「意味そのもの」を外部知識として注入するのではなく、**損失関数（次トークン予測など）を通じてデータから獲得される**という点である。したがって埋め込みの品質は、データ分布・語彙設計・最適化・正則化などの影響を強く受ける。

総括すると、one-hot は同一性の符号化としては明快だが、言語の類似性構造を表現できず、学習の一般化を助けない。一方で埋め込みは、トークンに連続空間上の座標を与えることで、意味の近さを幾何として取り込み、統計的強度の共有を可能にする。これが「分散表現」という発想の第一歩であり、次節で述べる分布仮説と共起の議論へ自然に接続するのである。
