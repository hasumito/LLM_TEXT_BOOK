## 5.4. 残差接続・LayerNorm・FFN：安定学習のための部品

TransformerはSelf-Attentionだけでは安定に深く積めない。実用的な深さで学習を成立させるために、**残差接続（residual connection）**、**正規化（Layer Normalization; LayerNorm）**、**位置ごとの全結合層（Feed-Forward Network; FFN）**が、各層の「足回り」として組み込まれている。本節では、それぞれが何をしており、なぜ学習を安定化するのかを整理する。

### 残差接続：深いネットワークで勾配を通すための迂回路

残差接続とは、ある変換 (F(\cdot)) の出力に入力を足し戻す機構である。入力を (x) とすると、残差ブロックの基本形は
[
y = x + F(x)
]
である。直観的には、層が学習すべきものを「ゼロからの完全な写像」ではなく、「入力からの差分（残差）」に変える。これが深層化に効く理由は二つある。

1つ目は**勾配の流れ**である。加算はヤコビアン（微分）が比較的素直で、少なくとも恒等写像の経路が残るため、深い層を通る際の勾配消失・爆発を緩和しやすい。2つ目は**最適化の地形**である。恒等写像に近い初期状態から段階的に機能を付け足せるため、学習初期に壊れにくい。

Transformerでは、Attentionサブ層とFFNサブ層のそれぞれに残差が入る。概念的には
[
x \leftarrow x + \text{Attention}(\cdot), \qquad
x \leftarrow x + \text{FFN}(x)
]
のように、更新を「加算」で積み重ねる設計になっている。

### LayerNorm：表現のスケールを整えて学習を壊れにくくする

LayerNormは、ベクトル表現の各次元（特徴量）を、**同一トークン内で正規化**する。隠れ状態 (x\in\mathbb{R}^{d}) に対し、平均 (\mu) と分散 (\sigma^2) を特徴次元方向に取り、
[
\text{LN}(x)=\gamma \odot \frac{x-\mu}{\sqrt{\sigma^2+\epsilon}} + \beta
]
とする（(\gamma,\beta) は学習可能なスケール・バイアス、(\epsilon) は数値安定化項である）。BatchNormと違い、ミニバッチ統計に依存しないため、系列長やバッチサイズが揺れる言語モデリングで扱いやすい。

LayerNormの役割は、ざっくり言えば**「各層の出力スケールが暴れるのを抑える」**ことにある。Self-Attentionは内積やSoftmaxを含み、FFNも非線形を含むため、層を重ねると活性の分布が偏りやすい。正規化はこの偏りを抑え、学習率や初期化に対する感度を下げ、深層でも発散しにくくする。

ここで重要なのが、LayerNormを**どこに置くか**である。歴史的にはTransformer原論文では各サブ層の後に置く形（しばしばPost-LNと呼ばれる）が使われたが、深層化では不安定になりやすいことが知られ、近年のLLMでは各サブ層の前に置くPre-LNが広く用いられる傾向がある。概念的には

* **Post-LN**：(y=\text{LN}(x+F(x)))
* **Pre-LN**：(y=x+F(\text{LN}(x)))

である。Pre-LNは、残差の恒等経路が正規化に邪魔されにくく、深いネットワークでの学習安定性が得られやすい。一方で、Post-LNは表現の正規化が各ブロック出力に強くかかるため、条件によっては表現が整いやすいという見方もある。どちらが「常に優れる」と断言できるものではなく、深さ、初期化、学習率スケジュール、正則化の設計と結びついた設計問題である。

### FFN：各トークンに非線形な「計算」を与える位置ごとの変換

Self-Attentionが得意なのは、トークン間の情報を混ぜることである。しかし注意は基本的に線形結合（Softmax重み付き和）を中心とした操作であり、それだけでは表現の変換能力が不足する。そこで各層に**位置ごとのFFN**が入る。これは「系列方向には混ぜず、各トークン独立に同じMLPを適用する」変換で、一般形は
[
\text{FFN}(x)=W_2,\phi(W_1 x + b_1)+b_2
]
である。ここで (W_1\in\mathbb{R}^{d_{\text{model}}\times d_{\text{ff}}})、(W_2\in\mathbb{R}^{d_{\text{ff}}\times d_{\text{model}}}) とし、通常 (d_{\text{ff}}) は (d_{\text{model}}) より大きい（例：4倍程度）ことが多い。いったん次元を膨らませて非線形変換を行い、元の次元に戻すことで、表現の変換能力を確保する。

活性化関数 (\phi) にはReLUやGELUなどが使われ、近年はゲート機構を取り入れたGLU系（例：SwiGLU）が採用されることも多い。ここでの本質は、FFNが**「混ぜる（Attention）」とは別に「計算する（非線形変換）」**役を担い、各層で表現を更新する駆動力になっている点である。注意だけのネットワークは情報伝播はできても、複雑な特徴抽出や条件分岐的な変換を作り込みにくい。FFNはその不足を補う。

### 3部品の協調：安定性と表現力の分業

残差接続・LayerNorm・FFNは、単体ではなく組として働く。粗い分業を言えば次の通りである。

* **残差接続**：深くしても学習が崩れにくい「足場」を作る
* **LayerNorm**：表現のスケールと分布を整え、最適化を扱いやすくする
* **FFN**：各トークン表現に非線形の変換能力（計算能力）を与える

ここで注意すべきは、これらは「唯一の正解の部品」ではない点である。LayerNormの代替としてRMSNormのように平均を引かない正規化が使われたり、残差に定数スケール（residual scaling）を掛けて発散を抑えたり、FFNをスパース化したりMoE（Mixture of Experts）に置換したりと、部品の交換は活発である。つまりTransformerの成功は、Self-Attentionという核に加えて、**深層最適化を成立させる実装上の工学的知恵が結晶化した結果**だと捉えるのが妥当である。Attentionが目立つ主役なら、残差・正規化・FFNは舞台を崩壊させないための構造材である。

