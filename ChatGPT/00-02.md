# 指示
LLMの仕組みについて解説する教科書を作成します。指定された節の解説文を作成してください。

# 今回作成する節
4.3. Seq2Seq と注意機構（attention）の登場

# 役割
あなたはLLMの専門家です。LLMに関する、歴史・仕組み・応用方法・限界・課題・最新の研究動向について深い知識が有ります。

# 条件
- 作成する教科書のタイトルは『LLMの仕組み』です。タイトル通り、LLMの仕組みを体系的に学べる教科書になります。
- LLMに関連する理論を中心に記載してください。ハンズオンは含める必要はありません。
- 事実に基づいた記述にしてください。
- 想定読者は、情報系の大学で情報関連の基礎を学んだ学部4年生です。
- 目次は全体構成を把握するための参考情報です。
- 読者は上から順番に読んでいくものとします。

# 目次
1. 序章：この教科書で学ぶこと
1.1. LLM（大規模言語モデル）とは何か：できること／できないこと
1.2. 「言語を扱う」とは何を意味するのか：確率・情報・意味
1.3. 学習・推論・評価の全体像（データ→学習→生成→運用）

2. 言語モデルの基礎：確率と言語
2.1. 言語の確率モデル：連鎖律と条件付き確率
2.2. n-gram からニューラルLMへ：疎性と一般化
2.3. 目的関数としての最尤推定と交差エントロピー
2.4. パープレキシティとその解釈

3. 分散表現：単語・文・意味の幾何
3.1. one-hot表現の限界と埋め込み（embedding）
3.2. 分布仮説と共起：意味を「使われ方」で近似する
3.3. 文脈化表現への道：固定表現から動的表現へ

4. Transformer前史：系列モデリングの系譜
4.1. RNNの基礎とBPTT（Backpropagation Through Time）
4.2. LSTM/GRU：長期依存とゲート機構
4.3. Seq2Seq と注意機構（attention）の登場

5. Transformerの骨格：自己注意の設計図
5.1. Self-Attention：Query/Key/Value と重み付け
5.2. Multi-Head Attention：表現の分解と再結合
5.3. 位置情報（Positional Encoding）：順序をどう埋め込むか
5.4. 残差接続・LayerNorm・FFN：安定学習のための部品

6. 生成の仕組み：デコーダLMとしてのLLM
6.1. 自回帰生成と因果マスク（causal masking）
6.2. トークン化：BPE/Unigram と語彙設計の理屈
6.3. 生成時の確率操作：温度・トップk・トップpの意味
6.4. 長文生成で起きる現象（反復・脱線・破綻）の構造

7. 学習の仕組み：最適化と勾配の現実
7.1. 勾配降下法と確率的最適化（SGD/Adam系）
7.2. 学習率スケジュール・ウォームアップ・重み減衰
7.3. バッチ・正則化・ドロップアウト：汎化との関係
7.4. 数値安定性：勾配爆発/消失と対策

8. スケーリング則：なぜ「大きいほど強い」のか
8.1. パラメータ・データ・計算量のトレードオフ
8.2. スケーリング則の観測事実とその含意
8.3. 最適計算配分（compute-optimal）の考え方
8.4. 破綻点とボトルネック（データ品質・評価限界）

9. 事前学習：自己教師あり学習の本質
9.1. 事前学習目的：次トークン予測の情報論的理解
9.2. データ分布とドメイン：何を学ぶかは何を食べたか
9.3. データ汚染（contamination）とリークの問題
9.4. カリキュラムと混合比：学習順序の影響

10. 適応と整合：Instruction Tuning とRLHFの理屈
10.1. 教師あり微調整（SFT）：指示追従の獲得
10.2. 人間の選好学習：報酬モデルとランキング損失
10.3. 強化学習による最適化（例：方策更新の考え方）
10.4. 整合の副作用：過度な無難化・知識の出力抑制

11. 表現と回路：内部で何が起きているのか
11.1. 注意重みの読み方：どこまで「説明」になるか
11.2. 特徴とニューロン：分散表現の解釈問題
11.3. 変換器の「回路」仮説：パターン認識の構造
11.4. 機械的解釈可能性（mechanistic interpretability）の射程

12. 推論時の拡張：記憶・検索・道具の統合
12.1. コンテキスト学習（in-context learning）の理解
12.2. RAG（検索拡張生成）：確率モデルとしての直観
12.3. ツール使用と計画：関数呼び出し的枠組み
12.4. 長期記憶と要約：情報圧縮がもたらす歪み

13. 評価：何を測ると「賢さ」が見えるのか
13.1. ベンチマークの設計：タスク・分布・難易度
13.2. 自動評価と人手評価：一致しない理由
13.3. 頑健性・汎化・外挿：分布外での失敗
13.4. 監査可能性：再現性・データ汚染検出・信頼性

14. 限界と失敗様式：ハルシネーションの構造
14.1. ハルシネーション：定義と発生メカニズム仮説
14.2. 推論の錯覚：それっぽさが正しさを偽装する
14.3. コンテキスト長と注意の限界（例：Lost in the Middle）
14.4. 安全性と敵対的入力：脱獄・プロンプト注入の理屈

15. 研究動向と未来：次の「仕組み」へ
15.1. 効率化：量子化・蒸留・スパース化・MoEの考え方
15.2. 学習の新潮流：合成データ、自己改善、データ設計
15.3. 信頼性強化：検証・校正・不確実性推定
15.4. 社会実装の論点：透明性・著作権・ガバナンス

# 出力形式
- 文体は「である調」にしてください。
- 専門用語を正確に使い、簡潔で明瞭な説明を心がけてください。
- 『4.3. Seq2Seq と注意機構（attention）の登場』の部分の文章のみを出力してください。
